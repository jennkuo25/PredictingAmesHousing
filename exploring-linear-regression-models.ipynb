{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## **Exploring linear regression: Predicting House Sale Prices**\nBy: Jennifer Kuo    |    Date: 1/8/19"},{"metadata":{"_uuid":"94d8cec156ba9629285d3196e79cd84007448591"},"cell_type":"markdown","source":"## Inspirations:\nThis is my first kernel as I start to immerse myself in these Kaggle challenges. My goal here is to go through the workflow of preparing the data for training the model, and then to test a few different linear regression implementations. I hope you find this notebook helpful for exploration!\n\nI've read through a few kernels for inspiration and insight, of which I'd like to highlight [Simple Linear Regression Models](https://www.kaggle.com/rbyron/simple-linear-regression-models) by Rocio Byron. She put together a great notebook with a clear framework."},{"metadata":{"_uuid":"1d135179fec572efdf86161347d433f0b11b9230"},"cell_type":"markdown","source":"## Outline:\n1. Understand the problem and the data\n2. Pre-processing (Data cleaning, normalizing the data, removing outliers) \n3. Feature Engineering\n4. Train and Test\n5. Compare Scores\n\n## 1. Understanding the problem\nWe would like to predict the SalePrice of houses based on features provided in the Ames Housing data set. Let's take a look at our data."},{"metadata":{"trusted":true,"_uuid":"7538241bf439df570d4644f5f5b0c2018cdb5471"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntestID = test['Id']\n\ndata = pd.concat([train.drop('SalePrice', axis=1), test], keys=['train', 'test'])\ndata.drop(['Id'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06b3f236a783d1eec27fdb6811564c8ba731d4c1"},"cell_type":"markdown","source":"## 2. Pre-processing:"},{"metadata":{"_uuid":"c9ba0922af354197526b7f741e2fa40dcfc034d5"},"cell_type":"markdown","source":"Data quality check:\n1. Are the years of columns within a reasonable range (ie. less than 2018)?\n2. Are the measurements within a reasonable range (ie. no negative numbers)?"},{"metadata":{"trusted":true,"_uuid":"b1efd0ce05e77e4f23f3c3294828e37fe39ab73b"},"cell_type":"code","source":"# Create list of years and metrics\nyears = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\nmetrics = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n         'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4c42c16078beb8185477266ae698f3337a42f73"},"cell_type":"code","source":"data[years].max()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab6d7426a335ac775ec814ba1805da059be5b454"},"cell_type":"markdown","source":"Looks like there is at least one entry in the GarageYrBlt column that is not an outlier, but rather a data entry error. The other year columns look fine. We will mask the error date with the date that the house was built. While this may not be correct, it is a better estimation as we don't have any other data to go off of. This is much more informative than keeping a spurious date that will be an outlier."},{"metadata":{"trusted":true,"_uuid":"a905d32d32478ea514d4253b441c64bf0a9dbf4e"},"cell_type":"code","source":"mask = (data[years] > 2018).any(axis=1) # take any index with illogical year value\ndata[mask]['GarageYrBlt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04941127da579e8c76a3c28cfc23aa65ef57ccce"},"cell_type":"code","source":"data.loc[mask, 'GarageYrBlt'] = data[mask]['YearBuilt']\ndata[mask]['GarageYrBlt']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"885aed4b52c0505bf9c3bc70abafbbb046540911"},"cell_type":"markdown","source":"Moving on! We'll split our data into numerical and categorical data for ease of manipulation as we observe trends, cleanse, and impute missing values. "},{"metadata":{"trusted":true,"_uuid":"b97985072f17c393bc235cba2e673969be6b80d4"},"cell_type":"code","source":"# Categorize features\nnumerical_feats = data.select_dtypes(include = ['float', 'integer']).columns\ncategorical_feats = data.select_dtypes(include = 'object').columns\nnumerical_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a5058d5fca4dc69c05ad7e0e4b448227a240bb3"},"cell_type":"code","source":"# List all columns indicating a grade (refer to data dictionary)\ngrades = ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n          'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\ndata[grades].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"885313db09c148d457cd8d96eed2d8fe193828a2"},"cell_type":"code","source":"# By looking at the data dictionary, we can assign the literal grades to a numerical value that will be more informative\nliteral = ['Ex', 'Gd', 'TA', 'Fa', 'Po']\nnum = [9, 7, 5, 3, 2]\n# Create a dictionary\nG = dict(zip(literal, num))\ndata[grades] = data[grades].replace(G)\ndata[grades].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee6b57135ac53fef323d6f5945e678bd23254945"},"cell_type":"markdown","source":"There are still NA values in these columns so we will have to deal with the missing values later."},{"metadata":{"trusted":true,"_uuid":"19dac0b7ce7f478048308fcff95ec3f539bae6f5"},"cell_type":"code","source":"# Create a list of column names from documentation that are *meant* to be categorical\nnominal_features = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b43d698e8bf556883a4c84360d40c493ffbf60"},"cell_type":"code","source":"# Explore which numerical columns should be converted to categorical\nnumerical_df = data.select_dtypes(include = ['float', 'integer'])\ncat_cols = []\nfor col in numerical_df.columns:\n    if col in nominal_features:\n        cat_cols.append(col)\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be60a3dc9b5eb40ac4d1656cc4893dbb396466d0"},"cell_type":"code","source":"# Adjust data type of variable MSSubClass to categorical\ndata['MSSubClass'] = data['MSSubClass'].astype('object', copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25065300958c4d74467e666b204293deeed06dad"},"cell_type":"markdown","source":"## Normalize the data"},{"metadata":{"_uuid":"127d91812fb73dc9335dc6826c175db8b1c5fbb9"},"cell_type":"markdown","source":"As we will be exploring different implementations of linear regression, we need to make sure that our data is normalized.  Let's first look at the distribution of the target variable, SalePrice."},{"metadata":{"trusted":true,"_uuid":"f362276a293b42053d90bbdcaae7691eb3c98999"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"061b5b2a949e5d40caf76a8ae8d1c64c1d4e7863"},"cell_type":"markdown","source":"The SalePrice variable is skewed right and non-normal. We will need to apply a log transformation to normalize the distribution."},{"metadata":{"trusted":true,"_uuid":"aa0da88febe3fd2b5b1a23d2848c6e46a030c9d9"},"cell_type":"code","source":"# Use the numpy fuction log1p which  applies log(1+x) to transform the target\nprice = np.log1p(train['SalePrice'])\n\n# Check the new distribution \nsns.distplot(price , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(price)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(price, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c6e154782856a936bb0150d6bc27327e4b6bafc"},"cell_type":"markdown","source":"Better! Now let's take a look at the other numerical features and whether they are skewed."},{"metadata":{"trusted":true,"_uuid":"9497ef0b176acc5218fbc830d7f4add4e294db96"},"cell_type":"code","source":"# Identify skewed continuous numerical features:\nskewed_feats = data.loc['train'][metrics].apply(lambda x: x.skew(skipna=True)) #compute skewness\nskewed_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a32445ce7cd5158ebefce65ff028212f3d16ea7"},"cell_type":"code","source":"# Get index of skewed features\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n# Log transform skewed features\ndata[skewed_feats] = np.log1p(data[skewed_feats])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"426d45f3734cbd3b1a1bd5d8656eda568292af7f"},"cell_type":"markdown","source":"## Explore data and remove outliers\n\nLet's plot some variables and see whether we can remove any obvious outliers."},{"metadata":{"trusted":true,"_uuid":"6b81c192348082dbf08318f6e5dd94a56fad1f76"},"cell_type":"code","source":"train['SalePrice'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4994b0209cacd0414939cb810860fc03f1f012"},"cell_type":"code","source":"# Add log-transformed SalePrice to train dataset\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ntrain['SalePrice'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41c4510bc853e149d437aa27ee756383dcbd8319"},"cell_type":"code","source":"# Observe correlation to determine which features to look at\nprint(\"Find most important features relative to target\")\nabs_corr_eff = train.corr()['SalePrice'].abs().sort_values(ascending = False)\nprint(abs_corr_eff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc344642707fb7befe3ecaa4924819e0e2a3e5a"},"cell_type":"code","source":"# Show columns with a correlation coefficient of larger than 0.5\nabs_corr_eff[abs_corr_eff > 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef47a92be932e9eca82b462f73b94741f0888a8"},"cell_type":"code","source":"# Look into OverallQual column since highly correlated\nsns.set_style(\"darkgrid\")\nfig, ax = plt.subplots()\nax.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQuality', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba9443ca5599aeebc95afdbfa5d67f836c9901d"},"cell_type":"markdown","source":"We can confirm there is a positive correlation with median sale price and higher overall quality as indicated by our correlation of columns and this visualization. There may be a few outliers like the listing of the house with overall quality of 4 that sold well, but the evidence is not strong enough to warrant removing."},{"metadata":{"trusted":true,"_uuid":"ae822d5fcd77380a3698bc7a92839498a3612715"},"cell_type":"code","source":"# Look into OverallQual column since highly correlated\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"886025dfc49a29cb728e37aade250cbce3e42baa"},"cell_type":"markdown","source":"There are two houses that list a GrLiv area of greater than 4000 that is selling much less than expected. We will remove these two listings as outliers so that they do not influence our model."},{"metadata":{"trusted":true,"_uuid":"35092f528d6c28c23bb5f7c47b22dade51397b02"},"cell_type":"code","source":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a7c211f7739220b0fbd83448b2cff83561f5cd3"},"cell_type":"code","source":"# Look into GarageCars column since highly correlated\nfig, ax = plt.subplots()\nax.scatter(x = train['GarageCars'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageCars', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1b81450225ac66b1634ad52b03b3aa3f9e73e8"},"cell_type":"markdown","source":"There are few listings with 4-car garages but we can observe the overall positive correlation of higher sale price for more car space in the garage. The next highly correlated variable is also relating to garages. We will have to reduce our variables since these could introduce colinearity as they are both related to garages."},{"metadata":{"_uuid":"af0d6fa835aeff29142913be3fbe836f7885c063"},"cell_type":"markdown","source":"## Deal with missing values"},{"metadata":{"trusted":true,"_uuid":"7a2f75e6e049449fa67b1a5ba03383d7df3f9b80"},"cell_type":"code","source":"# Check null values of numerical features\nnulls_num = pd.DataFrame(data[numerical_feats].isnull().sum().sort_values(ascending=False))\nnulls_num.columns = ['Null Count']\nnulls_num = nulls_num[nulls_num['Null Count'] > 0]\nnulls_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f99050b6092f0f00c87906a38f1679d76c5349c"},"cell_type":"code","source":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"589dd620205e6720e014d4f89fad8377c6ff6adc"},"cell_type":"markdown","source":"By reading the documentation, we can assume that the missing Basement, Garage, Masonry variables are missing because they do not exist."},{"metadata":{"trusted":true,"_uuid":"c9884f216177151adfa652797b0e393bc2d05cd3"},"cell_type":"code","source":"# Fill in missing values of numerical columns with 0\ndata[numerical_feats] = data[numerical_feats].fillna(0)\nnulls_num = pd.DataFrame(data[numerical_feats].isnull().sum().sort_values(ascending=False))\nnulls_num.columns = ['Null Count']\nnulls_num = nulls_num[nulls_num['Null Count'] > 0]\nnulls_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff9757c4042cf330fc6a3b45c5e2f5aeb4fb25f0"},"cell_type":"code","source":"# Let's look at PoolQC values to check distribution\ndata['PoolQC'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0927ac9737cc3864bf784cb1666d88979adec5d"},"cell_type":"markdown","source":"If we only have real data points for 10 listings, is this even a useful feature for our model?"},{"metadata":{"trusted":true,"_uuid":"0eb0f1a6d82106a920739bc664ca7d1101d04ecd"},"cell_type":"code","source":"# Check null values of categorical columns\nnulls_cat = pd.DataFrame(data[categorical_feats].isnull().sum().sort_values(ascending=False))\nnulls_cat.columns = ['Null Count']\nnulls_cat = nulls_cat[nulls_cat['Null Count'] > 0]\nnulls_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be0cb8a1d32369d4bb5819645f4ae9b5116c202c"},"cell_type":"code","source":"# Fill in missing value with NA = typical\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\n\n# Fill in missing value with most common value\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n\n# Fill in missing value with most common value\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n\n# Fill in missing value with most common value\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n\n# Substitute most common string for missing values\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\n# Fill in most common sale type for missing value\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07e54b1839c2a520c376e675e436faab1bc2105d"},"cell_type":"code","source":"# Check null values of categorical columns\nnulls_cat = pd.DataFrame(data[categorical_feats].isnull().sum().sort_values(ascending=False))\nnulls_cat.columns = ['Null Count']\nnulls_cat = nulls_cat[nulls_cat['Null Count'] > 0]\nnulls_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda5b09ba862a83099c91a792a07b88bcd0a9cee"},"cell_type":"code","source":"# Fill in missing values of categorical columns with '0'\ndata[categorical_feats] = data[categorical_feats].fillna('0')\nnulls_cat = pd.DataFrame(data[categorical_feats].isnull().sum().sort_values(ascending=False))\nnulls_cat.columns = ['Null Count']\nnulls_cat = nulls_cat[nulls_cat['Null Count'] > 0]\nnulls_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c6699e45a43d5ee6357e1499eb751bee4616454"},"cell_type":"markdown","source":"## 3.  Feature engineering"},{"metadata":{"trusted":true,"_uuid":"0b747bb429508e940bde1ef3878d73eaeb8a565c"},"cell_type":"code","source":"# Adding total sqfootage feature \ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37c61577d3e9e94af7956747477118878464cacf","trusted":true},"cell_type":"code","source":"# Engineer new features for years before sale, years since remodel\nyears_sold = data['YrSold'] - data['YearBuilt']  \nyears_remodeled = data['YrSold'] - data['YearRemodAdd'] \ndata['Years Before Sale'] = years_sold\ndata['Years Since Remodel'] = years_remodeled\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e81cc23440af524b146b27c26b96cf1c9eae02d7"},"cell_type":"markdown","source":"## Group categorical data with few samples\nSome categories of the categorical features are so unrrepresented in the dataset that drawing conclusions from them would lead to a noisy result. Instead, we will group those in one single category."},{"metadata":{"trusted":true,"_uuid":"a3280f535967e178e4dfe38ce486ad59c7d727a4"},"cell_type":"code","source":"train[categorical_feats].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d74e463ded4d1a5406b9336dfad5e6d283cafdc"},"cell_type":"code","source":"# Drop columns with less than 0.6 correlation with SalePrice\nfeats_to_drop = abs_corr_eff[abs_corr_eff < 0.6].index\nfor i in feats_to_drop:\n    if i in data.columns:\n        data = data.drop(i, axis=1)\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9329f107e8c4dcf937c254985542e73e15fc9413"},"cell_type":"code","source":"# Drop utilities column since no information added for predictive modeling\nfeats_to_drop_1 = ['Utilities', 'PoolQC', 'Alley', 'Street']\nfor i in feats_to_drop_1:\n    if i in data.columns:\n        data = data.drop(i, axis=1)\n    else:\n        pass\n# Drop old year columns that were replaced with 'Years Before Sale' and 'Years Since Remodel' feature\nfeats_to_drop_2 = ['YearBuilt', 'YearRemodAdd', 'YrSold']\nfor i in feats_to_drop_2:\n    if i in data.columns:\n        data = data.drop(i, axis=1)\n    else:\n        pass\n# Drop old year columns that were replaced with 'TotalSF' feature\nfeats_to_drop_3 = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\nfor i in feats_to_drop_3:\n    if i in data.columns:\n        data = data.drop(i, axis=1)\n    else:\n        pass\n# Drop Neighborhood column because too many unique values\nfeats_to_drop_4 = ['Neighborhood']\nfor i in feats_to_drop_4:\n    if i in data.columns:\n        data = data.drop(i, axis=1)\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80a26704930044d9faba98e5c968a96af336242f"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61f433c3ae167b7f726b50db8caae80f3bf96174"},"cell_type":"code","source":"cat_cols = data.select_dtypes(include = 'object').columns\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d5ccc5193affd4fae7e79321b60e37ebb2eb40"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5174957527a7a7fa9a1026b1c7e74b6337aac8fa"},"cell_type":"code","source":"finaldata = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce8e352a110a6437f4fa106d1c1037bb83df54df"},"cell_type":"code","source":"finaldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"508b8f31d87c2434ad81bf5ec1eee1d76335575c"},"cell_type":"markdown","source":"It would be cleaner (and less overfitting) to remove the '0' option as an encoding."},{"metadata":{"trusted":true,"_uuid":"819969c72262e6f0ea107f8fc7a7d55d31584c07"},"cell_type":"code","source":"bsmt = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n        'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath',\n        'BsmtHalfBath', \n        'TotalBsmtSF']\nfire = ['Fireplaces', 'FireplaceQu']\ngarage = ['GarageQual', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageCars', \n          'GarageArea', 'GarageYrBlt']\nmasn = ['MasVnrType', 'MasVnrArea']\nothers = ['Alley', 'Fence', 'PoolQC', 'MiscFeature']\n\nblack_list = bsmt + fire + garage + masn + others\nfor feat in finaldata.columns:\n    if ('_0' in feat) and (feat.split(\"_\")[0] in black_list):\n        finaldata.drop(feat, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5019de17840ccc20a9eb64711d919762ae69ec1b"},"cell_type":"code","source":"finaldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"662dfd44965037048efb1b82c9d0c162587ef5b0"},"cell_type":"markdown","source":"## 4. Split data, Train Model and Test"},{"metadata":{"trusted":true,"_uuid":"44ae2f0fd9a2262eca3c0ce20cfeb356a3caa425"},"cell_type":"code","source":"# Training/testing sets\nX_test = finaldata.loc['test']\nX_train = finaldata.loc['train']\n\ny_train = price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99c3db396ee5ee9a536321a2ad812630db063e46"},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dce042325b34f1d1be22769ef6700fac61dfbe3"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Create linear regression object\nLR = LinearRegression()\n\n# Train the model using the training sets\nLR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd88090ce110eeca7d0ff0bf87e5f261bf865158"},"cell_type":"code","source":"# Top influencers\nmaxcoef = np.argsort(-np.abs(LR.coef_))\ncoef = LR.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34c63caaf07f38051de0443b06cc7931d660728"},"cell_type":"markdown","source":"When we think about buying a house, the Roof Materials and MSZoning probably aren't the first thing we think about. The coefficient value represents the mean change in the response given a one unit change in the predictor. Would these features really have such a large impact? Our model is likely overfitted. Let's try using an L1 regularization technique, Lasso. This technique will perform feature selection for us and reduce the magnitude of the coefficients. I found this [article](https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression) very helpful."},{"metadata":{"trusted":true,"_uuid":"8bcdb041bd6b11c4458d4b56e2168ab03f425bb6"},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\n# Create linear regression object\nLs = LassoCV()\n\n# Train the model using the training sets\nLs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8829bfb28c7ac837a0b307eebaf0350c765dd65a"},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(Ls.coef_))\ncoef = Ls.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9d33d1afba0333f9c90569796c6ba240af4c2b9"},"cell_type":"markdown","source":"The features that have the largest coefficient are GrLiveArea and OverallQual, which make more sense when we think about the qualities we consider when buying a home. Now, let's experiment with an L2 regularization technique, Ridge regression."},{"metadata":{"trusted":true,"_uuid":"96fd70cde0da732324d419d2a5927be3c2dac685"},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\n# Create linear regression object\nRr = RidgeCV()\n\n# Train the model using the training sets\nRr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45dd7eac30a2670f9b6b6b61b13e0c4b1f0e8575"},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(Rr.coef_))\ncoef = Rr.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6495a44e6ba19af7e0d748854de1038540bfa9a"},"cell_type":"markdown","source":"The coefficients are consistent with the Lasso regression. We can also experiment with Elastic Net, which implements both the L1 and L2 penalty term by combining them algebraically."},{"metadata":{"trusted":true,"_uuid":"32bb28680aa57c660d4e444d7af23b773d42c8fd"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\n# Create linear regression object\nEN = ElasticNetCV(l1_ratio=np.linspace(0.1, 1.0, 5)) # we are essentially smashing most of the Rr model here\n\n# Train the model using the training sets\ntrain_EN = EN.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b7ae26b4f0474c78a8aaf58c7ca2952c83e387d"},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(EN.coef_))\ncoef = EN.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa5244fc9ad5b193d1d93c757178de68adcbc891"},"cell_type":"markdown","source":"## 5. Compare Scores "},{"metadata":{"trusted":true,"_uuid":"e3e8e7fd75dc3a2df885db4e59f339cbaa8837bd"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmodel = [Ls, Rr, EN]\nM = len(model)\nscore = np.empty((M, 5))\nfor i in range(0, M):\n    score[i, :] = cross_val_score(model[i], X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a3b5a74cedd1a5f4b50bf92356a5e27403eb73e"},"cell_type":"code","source":"# print out all scores\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66a9ce6ce732ed3538259c44ec9bac8758b77e92"},"cell_type":"code","source":"# get the average of scores for each model\nprint(score.mean(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e93552b8c705f609af82c6994be32f57e06d75b"},"cell_type":"markdown","source":"## Prepare submission file"},{"metadata":{"trusted":true,"_uuid":"958a9060c971e96459e4af610a93c09b57d7e745"},"cell_type":"code","source":"# My model based on ridge regression performed the best through cross-validation\nsubmit = pd.DataFrame({'Id': testID, 'SalePrice': np.exp(Rr.predict(X_test))})\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c1ccd36f1bd4d60331621756088289ac988514f"},"cell_type":"markdown","source":"## Future improvements"},{"metadata":{"_uuid":"33b9c00558ef8833b7d40d3265e95b2402397991"},"cell_type":"markdown","source":"* I would revisit the correlation coefficients and experiment with the cutoff threshold for the  variables I keep. \n* There are a lot of new features created from one-hot-encoding the categorical data. However, it might be better to bin the values into groups so that instead of 5 new categories being created, only 2 categories are or engineer new features out of combining categories."},{"metadata":{"trusted":true,"_uuid":"7325b0efc19e20d4940242606c5adb7230399cad"},"cell_type":"markdown","source":"If you read this and found it useful or have ideas for improvement, please share!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}